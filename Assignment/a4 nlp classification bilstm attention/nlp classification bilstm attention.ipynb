{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b4170d-2358-464d-bdef-adc1b4309607",
   "metadata": {},
   "source": [
    "# Assignment 4 : LSTM and Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe0f0c-1679-47a5-b575-523f464a9ab2",
   "metadata": {},
   "source": [
    "This assignment is composed of the following parts\n",
    "\n",
    "    1. LSTM and its variants\n",
    "        - Vanilla LSTM\n",
    "        - Coupled Gate LSTM\n",
    "        - Peephole LSTM\n",
    "        - BiLSTM\n",
    "    2. Attention Mechanism\n",
    "        - General Attention\n",
    "        - Self-Attention\n",
    "        \n",
    "Starting from BiLSTM part we will be working on a sequence classification model which has LSTM as the Encoder (and attention mechanisms before the output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43f45e-3826-48a2-b7de-a683f32e1661",
   "metadata": {},
   "source": [
    "### Code for preparing the dataset for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09aca745-714a-4604-9229-01cba31bf73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# #uncomment this if you are not using puffer\n",
    "# import os\n",
    "# os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "#https://github.com/pytorch/text/issues/1350\n",
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText('simple')\n",
    "\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "# vocab.get_itos() returns a list of strings (tokens), where the token at the i'th position is what you get from doing vocab[token]\n",
    "# get_vecs_by_tokens gets the pre-trained vector for each string when given a list of strings\n",
    "# therefore pretrained_embedding is a fully \"aligned\" embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6aab90-9ccd-449f-97d9-0374419b64ea",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60359109-acba-4baa-a2fb-438ff10c34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "hidden_dim = 256\n",
    "embed_dim = 300\n",
    "output_dim = 1\n",
    "\n",
    "pad_idx = vocab['<pad>']\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada68ab-e1c3-4907-93a8-05f0d22e0a96",
   "metadata": {},
   "source": [
    "### Code for preparing Train and Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703b8dbc-eaf5-402e-8327-8ecba507829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #++\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require length\n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.float64), pad_sequence(text_list, padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train_iter, test_iter = IMDB()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_loader = DataLoader(split_train_, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(split_valid_, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.RNN):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param) #<---here\n",
    "                \n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        predictions = predictions.squeeze(1)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        if i == 10:\n",
    "            break\n",
    "                \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text = text.to(device) #(batch_size, seq len)\n",
    "\n",
    "            predictions = model(text, text_length)\n",
    "            predictions = predictions.squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i == 10:\n",
    "                break\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb66181-ff2f-4d43-9bba-815ffa49acf5",
   "metadata": {},
   "source": [
    "## 1). LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43e65d-bd68-48dc-950b-09e53babd8d5",
   "metadata": {},
   "source": [
    "We have learned in class that LSTM was designed to avoid the long term dependency problem as well as to helps with the problem of vanishing and exploding gradients.\n",
    "\n",
    "The key to LSTM is the cell state and the 3 gates to 'protect' and 'control' the cell states.\n",
    "\n",
    "We will now look in to the components inside and implement them line by line :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661afcc-1cf6-4290-9a32-131220360554",
   "metadata": {},
   "source": [
    "**The expected shape of LSTM input is SHAPE : (bs, seq_len, input_dim)**\n",
    "\n",
    "For **EACH** time step of our sequence, these are the operations inside LSTM cell.\n",
    "\n",
    "The first step in our LSTM is to decide what information we’re going to throw away from the previous cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at $\\mathbf{h}_{t-1}$ and $\\mathbf{x}_t$, and outputs a number between 0 and 1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{b}_f)$\n",
    "\n",
    "The next step is to decide what new information we’re going to store in the cell state. This has two parts.\n",
    "First, a sigmoid layer called the “input gate layer” decides which values we’ll update.\n",
    "\n",
    "$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\ \\mathbf{h}_{t-1} + \\mathbf{U}_i \\ \\mathbf{x}_t + \\mathbf{b}_i)$\n",
    "\n",
    "Next, a tanh layer creates a vector of new 'candidate' values, $\\tilde{\\mathbf{c}}_t$ (aka. $\\mathbf{g}_t$), that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n",
    "\n",
    "$\\mathbf{g}_t = \\mathbf{tanh} \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t  + \\mathbf{b}_g)$\n",
    "\n",
    "It’s now time to update the old cell state, $\\mathbf{c}_{t-1}$, into the new cell state $\\mathbf{c}_t$. The previous steps already decided what to do, we just need to actually do it.\n",
    "We multiply the old state by f_t, forgetting the things we decided to forget earlier.\n",
    "Then we add $\\mathbf{i}_t \\circ \\mathbf{g}_t$. This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\tanh \\ (\\mathbf{c}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e380e-5fe5-4931-84ea-13a0e6d1e02d",
   "metadata": {},
   "source": [
    "In conclusion, these are the formula that we need to implement in our LSTM_cell class :\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\ \\mathbf{h}_{t-1} + \\mathbf{U}_i \\ \\mathbf{x}_t + \\mathbf{b}_i)$\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{g}_t = \\tanh \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t  + \\mathbf{b}_g)$\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\tanh \\ (\\mathbf{c}_t)$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathbf{h}_{t-1}$ is the hidden state from the previous time step [SHAPE : (bs, hidden_dim)] << no seq_len because it is only at time step t-1\n",
    "\n",
    "$\\mathbf{x}_t$ is the input of the current time step [SHAPE : (bs, hidden_dim)] << no seq_len because it is only at time step t\n",
    "\n",
    "$\\mathbf{W}$ are the weights that would be multiply with the hidden states [SHAPE : (hidden_dim, hidden_dim)]\n",
    "\n",
    "$\\mathbf{U}$ are the weights that would be multiply with the inputs [SHAPE : (input_dim, hidden_dim)]\n",
    "\n",
    "$\\mathbf{b}$ are the biases that would be added to the values before they are passed to sigmoid or tanh [SHAPE : (hidden_dim)]\n",
    "\n",
    "$\\circ$ is the Hadamard product as known as element-wise multiplication\n",
    "\n",
    "** $\\tilde{\\mathbf{c}}_t$ and $\\mathbf{g}_t$ can be used interchangeably"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c336be2-bf7f-4736-80b3-ab6442d114d1",
   "metadata": {},
   "source": [
    "### 1.1) Please implement the following LSTM_cell class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3f75d5-66c2-461b-9bd7-315a5dce12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # initialise the trainable Parameters\n",
    "        # These should be torch Parameter which is trainable ! (not just a simple tensor)\n",
    "        self.W_i = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_i = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.W_f = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_f = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.W_g = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_g = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_o = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x.shape =  (bs, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        bs, seq_len, _ = x.shape\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step \n",
    "        if init_states is None:\n",
    "            h_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        # For each time step of the input x, do ...\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            i_t = torch.sigmoid(h_t @ self.W_i + x_t @ self.U_i + self.b_i)\n",
    "            f_t = torch.sigmoid(h_t @ self.W_f + x_t @ self.U_f + self.b_f)\n",
    "            g_t = torch.tanh(h_t @ self.W_g + x_t @ self.U_g + self.b_g)\n",
    "            o_t = torch.sigmoid(h_t @ self.W_o + x_t @ self.U_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            output.append(h_t.unsqueeze(0)) # reshape h_t to (1, batch_size, hidden_dim), then APPEND to list 'output'\n",
    "        \n",
    "        output = torch.cat(output) # concatenate h_t of all time steps into SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0, 1) # just transpose to SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        return output, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a414477-0269-41b3-8454-d29dda4d6bd5",
   "metadata": {},
   "source": [
    "### Run this cell to check if your LSTM Cell can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8dc8192-84fc-45a4-bb76-5d331fa15798",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_LSTM_cell = LSTM_cell(embed_dim, hidden_dim)\n",
    "my_LSTM_cell.to(device)\n",
    "\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = my_LSTM_cell(test_data)\n",
    "\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df451c3-a5ce-47fe-aa55-e787896379f6",
   "metadata": {},
   "source": [
    "### Variants of LSTM\n",
    "\n",
    "Many variants of LSTM have been developed which are slightly different from Vanilla/Basic LSTM that we have just implemented above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecc7c5-63dc-456c-899a-83ce8cad1cb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### - Peephole LSTM\n",
    "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “Peephole Connections.” This means that we let all the gate layers look at the cell state.\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{P}_f \\ \\mathbf{c}_t + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\ \\mathbf{h}_{t-1} + \\mathbf{U}_i \\ \\mathbf{x}_t + \\mathbf{P}_i \\ \\mathbf{c}_t + \\mathbf{b}_i)$\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{P}_o \\ \\mathbf{c}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{g}_t = \\text{tanh} \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t  + \\mathbf{b}_g)$\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\tanh \\ (\\mathbf{c}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77c0e6-7199-410d-a625-804c281caf16",
   "metadata": {},
   "source": [
    "We can see that every gate now has $\\mathbf{c}_t$ as their input. And we also have 3 new parameters; $\\mathbf{P}_f$, $\\mathbf{P}_i$ and $\\mathbf{P}_o$ which has the same shape as $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fae08a-37be-4c58-9d4c-49adc9a9d811",
   "metadata": {},
   "source": [
    "#### - Coupled LSTM\n",
    "\n",
    "Another variation is to use Coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. The different is very simple. The input gate is now $(1 - \\mathbf{f}_t)$\n",
    "\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{i}_t = (1 - \\mathbf{f}_t)$\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{g}_t = \\text{tanh} \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t + \\mathbf{b}_g)$\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\text{tanh} \\ (\\mathbf{c}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df99c3-ade2-404c-9c74-a6a91156407d",
   "metadata": {},
   "source": [
    "### 1.2) Modify the 'LSTM_cell' class from 1.1 such that we can choose to use Vanilla / Peephole / Coupled LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556b9211-1d3b-4803-b6e6-e1422ff46f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <Put your modified 'new_LSTM_cell' class here>\n",
    "\n",
    "class new_LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, lstm_type: str):\n",
    "        super().__init__()\n",
    "        self.lstm_type = lstm_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # initialise the trainable Parameters\n",
    "        # These should be torch Parameter which is trainable ! (not just a simple tensor)\n",
    "        self.W_i = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_i = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.W_f = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_f = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.W_g = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_g = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "        self.U_o = nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "\n",
    "        if self.lstm_type == 'peephole':\n",
    "            self.P_f = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "            self.P_i = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "            self.P_o = nn.Parameter(torch.FloatTensor(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x.shape =  (bs, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        bs, seq_len, _ = x.shape\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step \n",
    "        if init_states is None:\n",
    "            h_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        # For each time step of the input x, do ...\n",
    "        for t in range(seq_len):\n",
    "            if self.lstm_type == 'vanilla':\n",
    "                x_t = x[:, t, :]\n",
    "                i_t = torch.sigmoid(h_t @ self.W_i + x_t @ self.U_i + self.b_i)\n",
    "                f_t = torch.sigmoid(h_t @ self.W_f + x_t @ self.U_f + self.b_f)\n",
    "                g_t = torch.tanh(h_t @ self.W_g + x_t @ self.U_g + self.b_g)\n",
    "                o_t = torch.sigmoid(h_t @ self.W_o + x_t @ self.U_o + self.b_o)\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "            elif self.lstm_type == 'coupled':\n",
    "                x_t = x[:, t, :]\n",
    "                f_t = torch.sigmoid(h_t @ self.W_f + x_t @ self.U_f + self.b_f)\n",
    "                i_t = torch.ones(f_t.shape).to(device) - f_t\n",
    "                g_t = torch.tanh(h_t @ self.W_g + x_t @ self.U_g + self.b_g)\n",
    "                o_t = torch.sigmoid(h_t @ self.W_o + x_t @ self.U_o + self.b_o)\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "            elif self.lstm_type == 'peephole':\n",
    "                x_t = x[:, t, :]\n",
    "                i_t = torch.sigmoid(h_t @ self.W_i + x_t @ self.U_i + c_t @ self.P_i + self.b_i)\n",
    "                f_t = torch.sigmoid(h_t @ self.W_f + x_t @ self.U_f + c_t @ self.P_f + self.b_f)\n",
    "                g_t = torch.tanh(h_t @ self.W_g + x_t @ self.U_g + self.b_g)\n",
    "                o_t = torch.sigmoid(h_t @ self.W_o + x_t @ self.U_o + c_t @ self.P_o + self.b_o)\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "                \n",
    "            output.append(h_t.unsqueeze(0)) # reshape h_t to (1, batch_size, hidden_dim), then APPEND to list 'output'\n",
    "        \n",
    "        output = torch.cat(output) # concatenate h_t of all time steps into SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0, 1) # just transpose to SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        return output, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92273dc5-957c-40b5-98e5-61c64bbcc869",
   "metadata": {},
   "source": [
    "### Run this cell to check if all types of your LSTM Cells can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d11723c-88e2-4c16-bc13-5bf249e379f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vanilla_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla').to(device)\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = Vanilla_LSTM_cell(test_data)\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])\n",
    "\n",
    "Coupled_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'coupled').to(device)\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = Coupled_LSTM_cell(test_data)\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])\n",
    "\n",
    "Peephole_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'peephole').to(device)\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = Peephole_LSTM_cell(test_data)\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f923e60-2fb7-455d-af05-8aa95a64c816",
   "metadata": {},
   "source": [
    "### BiLSTM model for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97361a-4b6a-46e7-9d0a-a19cbb8e614e",
   "metadata": {},
   "source": [
    "We now have the basic variants of LSTM cells. But what about Bidirectional LSTM. How do we implement that?\n",
    "\n",
    "The answer is simple. We create **2 LSTM cells** then pass our normal input to one of them, and pass the **flipped** input to the other. (reverse the order of sequence)\n",
    "\n",
    "Then we take the last hidden state from the 2 LSTM (one would be the hidden state at the last word of the sentence and another at the first word of the sentence) and concatenate them. Like this we have information of the sequence from both directions!\n",
    "\n",
    "Formally these are the formula\n",
    "\n",
    "$\\overrightarrow{\\mathbf{h}}_t = LSTM(\\mathbf{x}_t, \\overrightarrow{\\mathbf{h}}_{t-1})$\n",
    "\n",
    "$\\overleftarrow{\\mathbf{h}}_t = LSTM(\\mathbf{x}_t, \\overleftarrow{\\mathbf{h}}_{t+1})$\n",
    "\n",
    "$\\mathbf{h}_t = \\sigma(\\mathbf{W}_y[\\overrightarrow{\\mathbf{h}}_t ; \\overleftarrow{\\mathbf{h}}_{t}] + \\mathbf{b}_y )$\n",
    "\n",
    "Then we should pass $\\mathbf{h}_t$ to another Linear Layer to get the output for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07808fe-4bd0-40d7-a88b-f9ebc60ebd07",
   "metadata": {},
   "source": [
    "### 1.3) Implement the following 'BiLSTM_model' class\n",
    "\n",
    "It should be a model for sequence classification which only has BiLSTM as its encoder and a Linear Layer for outputting the binary classification class decision.\n",
    "\n",
    "**( Let's use our 'vanilla' LSTM_cell)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffa46a8-a5c4-485c-81b2-8f43d6792046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_model(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        \n",
    "        self.forward_lstm   = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        self.backward_lstm  = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        \n",
    "        # These should be torch Parameters\n",
    "        self.W_h = nn.Parameter(torch.FloatTensor(hidden_dim * 2, output_dim)) # SHAPE : (hidden_dim * num_directions, output_dim)\n",
    "        self.b_h = nn.Parameter(torch.FloatTensor(output_dim)) # SHAPE : (hidden_dim * num_directions, output_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(output_dim, output_dim)\n",
    "    \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded      = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        embedded_flip = torch.flip(embedded, [1]) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        output_forward, (hn_forward, cn_forward)    = self.forward_lstm(embedded) # hn_forward SHAPE : (batch_size, hidden_dim)\n",
    "        output_backward, (hn_backward, cn_backward) = self.backward_lstm(embedded_flip) # hn_backward SHAPE : (batch_size, hidden_dim)\n",
    "\n",
    "        concat_hn = torch.cat((hn_forward, hn_backward), 1)  # SHAPE : (batch_size, hidden_dim * num_directions)\n",
    "        ht        = torch.sigmoid(concat_hn @ self.W_h + self.b_h) # SHAPE : (batch_size , output_dim)    \n",
    "            \n",
    "        return self.fc(ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1d990-aa65-4cb1-a436-a25f73a91e3c",
   "metadata": {},
   "source": [
    "### Run this cell to show that you can train the model with your BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211e0b32-cf2d-424e-9e85-98242624cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.011 | Train Acc: 0.79%\n",
      "\t Val. Loss: 0.200 |  Val. Acc: 15.08%\n",
      "Epoch: 02 | Train Loss: 0.011 | Train Acc: 0.78%\n",
      "\t Val. Loss: 0.200 |  Val. Acc: 14.84%\n",
      "Epoch: 03 | Train Loss: 0.011 | Train Acc: 0.69%\n",
      "\t Val. Loss: 0.202 |  Val. Acc: 14.06%\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(input_dim, embed_dim, hidden_dim, output_dim).to(device)\n",
    "bilstm.apply(initialize_weights)\n",
    "bilstm.embedding.weight.data = fast_embedding\n",
    "\n",
    "optimizer = optim.Adam(bilstm.parameters(), lr=lr) #<----changed to Adam\n",
    "criterion = nn.BCEWithLogitsLoss() #combine sigmoid with binary cross entropy\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(bilstm, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(bilstm, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "del bilstm\n",
    "del optimizer\n",
    "del criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aaecfd-567d-45ad-8d34-a65dda9f1935",
   "metadata": {},
   "source": [
    "As you can see, the 6 equations in our LSTM cell means there are at least 6 matrix multiplication operations for each time step in each of our input sequence, which is A LOT. **But pytorch has the optimized version of LSTM which is much more efficient so let's use that in the next parts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3eace-4d78-4546-9819-9fb30ec29499",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.) Attention Mechanism\n",
    "The attention mechanism was first born to help memorize long source sentences in neural machine translation (NMT). Rather than building a single context vector out of the encoder’s last hidden state, the attention mechanism creates shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\n",
    "While the context vector has access to the entire input sequence, we don’t need to worry about forgetting. The alignment between the source and target is learned and controlled by the context vector. Essentially the context vector consumes three pieces of information:\n",
    "\n",
    "    - encoder hidden states\n",
    "    - decoder hidden states\n",
    "    - alignment between source and target\n",
    "    \n",
    "This is the same mechanism that we have learned in class, which is actually called 'Cross Attention'.\n",
    "\n",
    "However, in this assignment we are making a classification model so we only have the encoder hidden states and our target would be the class decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddeef0-9bf1-40f6-a01a-d32d849bb150",
   "metadata": {},
   "source": [
    "### General Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acd0fb-3dd2-4528-aa1e-98802543c95f",
   "metadata": {},
   "source": [
    "First, we will be creating an LSTM + General Attention model for classification. Which will be a little bit different from what Prof has taught in class, such that we only have an encoder and we don't have any decoder.\n",
    "In this task, we are going to use LSTM as the encoder and use General Attention before we output the class decision.\n",
    "\n",
    "Our General Attention mechanism is going to capture how the last encoder hidden state (aka. the 'queries') 'relates' to the other hidden states in the sequence (a.k.a. the 'keys'). ( how much our classification decision is related to each of the hidden states)\n",
    "Then we will scale the output (a.k.a. the 'values') according to the Attention Weights (computed from the Alignment Scores), in order to retain focus on words that are relevant to the query. In doing so, it produces an attention output that we will input to a fully connected layer for the result of our classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600dd8d-4f52-4901-8427-940777e0f683",
   "metadata": {},
   "source": [
    "**These are the steps we need to implement :**\n",
    "\n",
    "We will pass our data through LSTM first then pass the outputs of LSTM to the General Attention mechanism.\n",
    "    \n",
    "**1. Get the components we need for our Attention Mechanism ('query', 'keys' and 'values')**\n",
    "- Get the last encoder hidden states ($\\mathbf{h}_N$) = last hidden state of last LSTM layer\n",
    "        - Hint : can be found in 'hn'\n",
    "        - Should be of shape [bs, hidden dim * num_directions]\n",
    "        - a.k.a. 'query'\n",
    "- Get the hidden states of every time step from the last layer of LSTM ($\\mathbf{H}$)\n",
    "        - Hint : can be found in 'output' \n",
    "        - Should be of shape  [bs, seq len, hidden_dim * num_directions]\n",
    "        - a.k.a. 'keys'\n",
    "        - This will be matched with our h_t to get the Attention Scores.\n",
    "- Get the hidden states of every time step from the last layer of LSTM ($\\mathbf{H}$)\n",
    "        - Hint : can be found in 'output' \n",
    "        - Should be of shape  [bs, seq len, hidden_dim * num_directions]\n",
    "        - a.k.a. 'values'\n",
    "        - This will be weighted by Attention Weights to get the Context.\n",
    "** In our case, we are implementing Attention in Classification model so our 'keys' and 'values' are the same thing\n",
    "\n",
    "**2.  Calculate Alignment Scores:**\n",
    "\n",
    "Calculate the Alignment Scores by matching the **'query'** with each of the **'keys'**. This matching operation is computed as the **dot product** of our specific 'query' with each of the hidden states or the 'key' vector. This is to get the scores of how 'related' the 'query' is to each 'key' or each hidden state.\n",
    "\n",
    "$\\mathbf{e}_t = [\\mathbf{h}_N^T \\ \\mathbf{h}_1,  \\mathbf{h}_N^T \\ \\mathbf{h}_2,  ..., \\mathbf{h}_N^T \\ \\mathbf{h}_N] \\in \\mathbb{R}^N $\n",
    "\n",
    "where\n",
    "\n",
    "$ \\mathbf{h}_1, ..., \\mathbf{h}_N \\in \\mathbb{R}^h ; \\mathbf{H} \\in \\mathbb{R}^{N,h}$\n",
    "\n",
    "Hint : We can multiply our 'query' with all of the 'keys' at once by using the matrix form of the 'query' ($\\mathbf{H}$) ( we have to keep shape of batch size at the first dimension so **torch.bmm** might come in handy !)\n",
    "\n",
    "Hint2: Alignment Scores should be of shape :  [batch_size, seq_len, 1]\n",
    "\n",
    "**3. Calculate Attention Weights :**\n",
    "\n",
    "We pass the Alignment Scores through a **softmax** operation to convert the scores into probabilities called the 'Attention Weights'\n",
    "This method is called **soft-attention** which help make the model smooth and differentiable.\n",
    "\n",
    "$\\alpha_t = \\text{softmax}(\\mathbf{e}_t) \\in \\mathbb{R}^N$\n",
    "\n",
    "Hint : our softmaxed Attention Weights should still have the same shape as Alignment Score\n",
    "\n",
    "**4. Calculate Context Vector :**\n",
    "\n",
    "Use the Attention Weight to scale the output **'values'** to get the 'context vector'. In this example, the 'values' is the same as the 'keys' which is the hidden states of every time step from the last layer of LSTM. A context vector is a **weighted sum** of the value vectors, V_ki.\n",
    "\n",
    "$ \\mathbf{c}_t = \\mathbf{H}^T \\ \\alpha_t \\in \\mathbb{R}^h $\n",
    "\n",
    "Hint : Again, we can use the matrix form to get the weighted sum in one operation.\n",
    "The resulting context should be of shape [bs, hidden_size * num_directions]\n",
    "\n",
    "**5. Finally, we use this Context Vector as the output of our Attention Mechanism**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3611a36-2cf8-4e7e-8d9d-cdfc1c410bb1",
   "metadata": {},
   "source": [
    "### 2.1) Implement the following LSTM + General Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afe0fd8-8726-4a2d-9244-a41455180b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTM_GAtt(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # let's use pytorch's LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Linear Layer for binary classification \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def attention_net(self, lstm_output, hn):\n",
    "        \n",
    "        h_t      = torch.clone(hn) # last hidden state of last layer (Hint : can be found in 'hn') >> SHAPE : [bs, hidden dim * num_directions]\n",
    "        # use torch.clone to copy tensors safely\n",
    "        H_keys   = torch.clone(lstm_output) # hidden states of every time step from the last layer of LSTM (Hint : can be found in 'output' ) >> SHAPE : [bs, seq len, hidden_dim * num_directions]\n",
    "        H_values = torch.clone(lstm_output) # hidden states of every time step from the last layer of LSTM (Hint : can be found in 'output' ) >> SHAPE : [bs, seq len, hidden_dim * num_directions]\n",
    "        \n",
    "        alignment_score   = torch.bmm(H_keys, h_t.unsqueeze(dim=2)) # SHAPE : (bs, seq_len, 1)\n",
    "        \n",
    "        soft_attn_weights = torch.softmax(alignment_score, dim=1) # SHAPE : (bs, seq_len, 1)\n",
    "        \n",
    "        context           = torch.bmm(torch.transpose(H_values, 1, 2), soft_attn_weights) # SHAPE : (bs, hidden_size * num_directions)\n",
    "        \n",
    "        return context.squeeze()\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # This is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "\n",
    "        attn_output = self.attention_net(lstm_output, hn)\n",
    "        \n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2d46e-e216-475e-a155-a650d527d7b0",
   "metadata": {},
   "source": [
    "### Run this cell to show that you can train the model with your LSTM_GAtt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6c9cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117f2e9f-b5f1-4601-98bb-2c99c4a37b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.010 | Train Acc: 0.72%\n",
      "\t Val. Loss: 0.190 |  Val. Acc: 14.30%\n",
      "Epoch: 02 | Train Loss: 0.010 | Train Acc: 0.78%\n",
      "\t Val. Loss: 0.190 |  Val. Acc: 14.14%\n",
      "Epoch: 03 | Train Loss: 0.010 | Train Acc: 0.75%\n",
      "\t Val. Loss: 0.191 |  Val. Acc: 13.36%\n"
     ]
    }
   ],
   "source": [
    "g_attmodel = LSTM_GAtt(input_dim, embed_dim, hidden_dim, output_dim).to(device)\n",
    "g_attmodel.apply(initialize_weights)\n",
    "g_attmodel.embedding.weight.data = fast_embedding\n",
    "\n",
    "optimizer = optim.Adam(g_attmodel.parameters(), lr=lr) #<----changed to Adam\n",
    "criterion = nn.BCEWithLogitsLoss() #combine sigmoid with binary cross entropy\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(g_attmodel, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(g_attmodel, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "del g_attmodel\n",
    "del optimizer\n",
    "del criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed6151-1a0d-4b6b-b025-8ca646368da6",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism\n",
    "\n",
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence.\n",
    "The self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n",
    "\n",
    "It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a7774-6e13-49a0-8edc-84005f0bd284",
   "metadata": {},
   "source": [
    "You might have noticed from the previous part that the are 3 main vector/matrix in the attention mechanism, which are 'queries', 'keys' and 'value'.\n",
    "Self-attention also need the same elements but we only have 'self' for the model to consider so 'queries', 'keys' and 'value' is all made from our input. Other steps are very similar to General Attention.\n",
    "\n",
    "Same with the previous part, we will pass our data through LSTM first then pass the outputs of LSTM to the Self Attention mechanism.\n",
    "\n",
    "**1. Get the components we need for our Attention Mechanism**\n",
    "\n",
    "Make **3 copies** of $ \\mathbf{H} $ (hidden states of every time step from the last layer of LSTM)\n",
    "\n",
    "        - Hint : can be found in 'output' \n",
    "        - Should be of shape  [bs, seq len, hidden_dim * num_directions]\n",
    "\n",
    "**2. Initialize 3 Linear Layers :**\n",
    "\n",
    "    - Initialize 3 Linear Layer called 'lin_Q', 'lin_K', 'lin_V'\n",
    "    \n",
    "    - input_dim = hidden_dim * num_direction\n",
    "    - output_dim = hidden_dim * num_direction\n",
    "    \n",
    "**3. Pass each copy of lstm_output through each of the Linear Layer.**\n",
    "    \n",
    "Pass each copy of $\\mathbf{H}$ through each of the Linear Layer so that we have learnable weights for generating the queries, keys and values\n",
    "\n",
    "$ \\mathbf{Q} = \\mathbf{H}^T \\ \\mathbf{W}_q + \\mathbf{b}_q \\in \\mathbb{R}^{N,h} $\n",
    "\n",
    "$ \\mathbf{K} = \\mathbf{H}^T \\ \\mathbf{W}_k + \\mathbf{b}_k \\in \\mathbb{R}^{N,h} $\n",
    "\n",
    "$ \\mathbf{V} = \\mathbf{H}^T \\ \\mathbf{W}_v + \\mathbf{b}_v \\in \\mathbb{R}^{N,h} $\n",
    "\n",
    "*Hint: Expected SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "\n",
    "**4. Calculate Alignment Scores:**\n",
    "\n",
    "    - Matching the 'query' with the 'keys'.\n",
    "    \n",
    "$\\text{AlignmentScore} = \\mathbf{Q} \\ \\mathbf{K}^T \\in \\mathbb{R}^{N,N}$ \n",
    "\n",
    "    - Hint: Our 'query' and 'keys' are both matrix so you might want to use 'torch's matrix multiplication'.\n",
    "    - Expected SHAPE : (bs, seq_len, seq_len) because we want to match each time step in self with each time step of itself\n",
    "    \n",
    "**5. Padding Mask**\n",
    "\n",
    "Since there are many padding tokens in our input sequence. It would be inefficient to leave them as is. Please implement 'pad_mask' which will replace the Alignment Scores with -1e9 where the input sequence is the padding token.\n",
    "\n",
    "**Skipping this step will not affect the next parts :)\n",
    "\n",
    "**6. Calculate Attention Weights :**\n",
    "\n",
    "    - Pass the Alignment Scores through Softmax\n",
    "    \n",
    "$\\text{Attention Weights} = \\text{softmax}(\\text{AlignmentScore}) \\in \\mathbb{R}^{N,N} $ \n",
    "\n",
    "**7. Calculate the Context Vector :**\n",
    "\n",
    "    - Multiply the Attention Weights with the 'values' to get the Context vector of SHAPE : (bs, seq_len, hidden_dim * num_directions)\n",
    "    - Then do 'Sequence Length Reduction' to aggregate the dimension of seq_len into 1.\n",
    "    - You can choose between averaging or sum.\n",
    "    - Finally, Context vector should have SHAPE : (bs, hidden_dim * num_directions)\n",
    "    \n",
    "$\\text{Context Vector} = \\text{Attention Weights} \\ \\mathbf{V} \\in \\mathbb{R}^{h} $ \n",
    "    \n",
    "    \n",
    "**8. Finally, we use this Context Vector as the output of our Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc68b1-bc4c-4045-8e16-3f5478301f7c",
   "metadata": {},
   "source": [
    "### 2.2) Implement the following LSTM + Self Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ab926b-b666-480c-a76d-5fa1712d2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SelfAtt(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim, len_reduction):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.len_reduction = len_reduction\n",
    "        \n",
    "        # let's use pytorch's LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        \n",
    "        # Linear Layer for binary classification \n",
    "        self.fc    = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def self_attention_net(self, lstm_output):\n",
    "        \n",
    "        Hq = torch.clone(lstm_output)\n",
    "        Hk = torch.clone(lstm_output)\n",
    "        Hv = torch.clone(lstm_output)\n",
    "        \n",
    "        Q = self.lin_Q(Hq)\n",
    "        K = self.lin_K(Hk)\n",
    "        V = self.lin_V(Hv)\n",
    "\n",
    "\n",
    "        alignment_score = torch.bmm(Q, torch.transpose(K, 1, 2)) # SHAPE : (bs, seq_len, seq_len)\n",
    "                \n",
    "        # apply padding mask \n",
    "        # <your_code_here>\n",
    "        \n",
    "        soft_attn_weights = self.softmax(alignment_score)\n",
    "        \n",
    "        context = torch.bmm(soft_attn_weights,V) # SHAPE : (bs, seq_len, hidden_dim * num_directions)\n",
    "        \n",
    "        # Do Mean or Sum len reduction\n",
    "        if self.len_reduction == \"mean\":\n",
    "            len_reduced_context = torch.mean(context, dim=1)\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            len_reduced_context =  torch.sum(context, dim=1)\n",
    "        \n",
    "        return len_reduced_context\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # This is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "\n",
    "        attn_output = self.self_attention_net(lstm_output)\n",
    "        \n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35abb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effea17c-6bda-48fa-9406-d836047670bb",
   "metadata": {},
   "source": [
    "### Run this cell to show that you can train the model with your LSTM_SelfAtt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bc5743-3def-49bb-ae49-b3e6a91df790",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 140.00 MiB (GPU 0; 3.82 GiB total capacity; 2.33 GiB already allocated; 66.12 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4876/3389069749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself_attmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_SelfAtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_reduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mself_attmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialize_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mself_attmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_attmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#<----changed to Adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 0; 3.82 GiB total capacity; 2.33 GiB already allocated; 66.12 MiB free; 2.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "self_attmodel = LSTM_SelfAtt(input_dim, embed_dim, hidden_dim, output_dim, len_reduction='mean').to(device)\n",
    "self_attmodel.apply(initialize_weights)\n",
    "self_attmodel.embedding.weight.data = fast_embedding\n",
    "\n",
    "optimizer = optim.Adam(self_attmodel.parameters(), lr=lr) #<----changed to Adam\n",
    "criterion = nn.BCEWithLogitsLoss() #combine sigmoid with binary cross entropy\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(self_attmodel, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(self_attmodel, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "del self_attmodel\n",
    "del optimizer\n",
    "del criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffeec41-b9d8-4dad-bec8-296e81640236",
   "metadata": {},
   "source": [
    "### 2.3) In our implementation we have used 'Dot-Product' to calculate our Alignment Scores. Give 2 examples of Alignment score functions and their formula (other than the ones in our lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12680244-e38e-4328-b0fc-994508800acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb0a25-0673-4838-acfe-63be7c1a41ad",
   "metadata": {},
   "source": [
    "### 2.4) As explained in the implementation we used softmax to calculate the 'Soft' Attention weights. Explain how to 'Hard' attention works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc0611-de91-460d-a956-e41b5002f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be24b0a-da93-47fe-a887-c7dfdaa82523",
   "metadata": {},
   "source": [
    "#### Reference & Further Readings:\n",
    "\n",
    "LSTM :\n",
    "\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0\n",
    "\n",
    "Attention : \n",
    "\n",
    "- https://arxiv.org/pdf/1409.0473.pdf\n",
    "- https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "- https://machinelearningmastery.com/the-attention-mechanism-from-scratch/#:~:text=In%20essence%2C%20when%20the%20generalized,the%20others%20in%20the%20sequence.\n",
    "- https://blog.floydhub.com/attention-mechanism/#bahdanau-att\n",
    "- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "- https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba5d6a-f5ac-4bbf-91ae-dd1c16916ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30b1d7f8aaabcde0ad92b80ea6c55de77136947a2c0039d0ec9dbe5c2a6a12e0"
  },
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
